\section{Experimental results}
\label{sec:Experiments}
\subsection{Variational auto-encoders}

A very interesting and powerful application of VI is the so-called Variational Auto-Encoder (VAE), which was introduced by \textcite{Kingma2014} and \textcite{Rezende2014} independently. VAEs are used to estimate the probability density of a set of observations $\{x_i\}_{i=1, \dots, N}$ by assuming the existence of a more concise latent representation or \textit{encoding} $z_i$ for each observed point. This model can be trained by optimizing the lower bound $\mathcal{L}$ on the marginal likelihood $p(x_i)$ not only w.r.t.\ the parameters of the posterior approximation, but also w.r.t.\ the parameters of a generative model for $p(x_i, z_i)$ at the same time. Here, the generative model usually consists of a fixed prior for the latent variables $\pi(z_i)$ and a conditional distribution or \textit{decoder} $p(x_i|z_i)$ to be learnt. Correspondingly, the posterior approximation $q(z_i|x_i)$ is referred to as the \textit{encoder}.

In the following, we apply HMCVI to this model by enhancing the encoder through the addition of HMC steps and maximizing the auxiliary lower bound $\mathcal{L_\textrm{aux}}$. This should lead to an encoding closer to the best possible encoding given by the true but intractable posterior $p(z_i|x_i)$. In the HMC steps the generative model induces the energy surface on which the motion of particles is simulated. Therefore, in order to avoid numerical instabilities and unexpected behaviour, it is recommended to choose $p(x|z)$ to be smooth .

\subsection{The dataset and the effects of data binarization}
\label{sec:Dataset}
A common benchmark dataset for machine learning problems is the MNIST dataset compiled by \textcite{LeCun1998}, which consists of a total of 70000 $28 \times 28$ pixel images of handwritten digits. The usual modelling approach for probability density estimation of these images is to assume that the pixels follow Bernoulli distributions, so that sampled images are binary, i.e.\ only contain the values 0 (black) and 1 (white). However, while the underlying images were binary, the images in the dataset contain grey-scales due to the anti-aliasing techniques applied during the normalization preprocessing. To deal with this gap between the binary bi-level modelling approach and the smoother multilevel dataset, several strategies are in use.

The most obvious approach is to directly use the unbinarized original dataset (fig.~\ref{fig:MNISTBinarizationComparison}, left), where pixel values range from $0$ to $255/256$ (with $256$ levels). A drawback of this method is its incompatibility with the assumption of a Bernoulli distribution, which leads to a lower likelihood of the model.
% presumably done by \parencite{Kingma2014}
To avoid this incompatibility, it is necessary to binarize the images in the dataset. One way to do this is by applying a threshold to the pixel values, so setting the pixel to $1$, if its value is greater or equal to $0.5$, and to $0$ otherwise. This results in very clear images (fig.~\ref{fig:MNISTBinarizationComparison}, middle) and correspondingly a extremely high likelihood for most models. Although this is a very intuitive binarization strategy, it is rarely used in practice.

The most common binarization strategy for MNIST is stochastic binarization, which was introduced by \textcite{Salakhutdinov2008} and has become a standard benchmark for density estimation algorithms \parencite{Salimans2014,Rezende2014,Gregor2015}. Here, each pixel is randomly set to $1$ with the probability given by its value and to $0$ otherwise, so that taking the average over many draws from the same image returns the original unbinarized image. This procedure can produce somewhat unrealistic digits, for example with gaps, but still the digits are clearly recognizable (fig.~\ref{fig:MNISTBinarizationComparison}, right). A beneficial side-effect of this randomization is that it counteracts over-fitting to the training set, since the training images appear in many different forms, effectively creating a much larger dataset. In this sense, stochastic binarization is similar to dropout regularization \parencite{Hinton2012}. To capitalize on these benefits it is essential to redraw from the training data at the beginning of every epoch. Similarly, multiple draws from the validation and test sets should be used for model selection and evaluation in order to obtain robust results. 

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figures/binarization_example.pdf}
\caption{Comparison of different binarization strategies on MNIST. The original (left) containing grey-scales was binarized using thresholding (middle) and stochastic binarization (right).}
\label{fig:MNISTBinarizationComparison}
\end{figure}

\subsection{Model specifications}
\label{sec:ModelSpecifications}
We will evaluate HMCVI on the MNIST dataset with stochastic binarization for better comparability. The training data was resampled as described above before each iteration. For the validation and test set five random draws from the unbinarized sets were used. The HMCVI algorithm was implemented in python using the package Theano \parencite{Bergstra2010, Bastien2012}. All models were trained for several thousand epochs using Adam \parencite{Kingma2015} integrated with Theano by the package climin \parencite{Bayer2015}. Adam was run with the default parameters except for the step size, which was set to $10^{-4}$ or $5 \cdot 10^{-5}$.

In all experiments the decoding model $p(x_i|z_i)$ consisted of a conditionally independent Bernoulli distribution over the pixels with the rates given by a fully connected neural network with the latent variables as input. This network had two hidden layers with 200 neurons each and softplus ($\log(1 + \exp(x))$) activations. In the output layer the element-wise sigmoid activation function was applied. Similarly, for the initial encoder model $q_0(z_i|x_i)$ a multivariate normal distribution with diagonal covariance was used, where the parameters were given by a second neural network taking the observed variables as inputs. Again, two hidden layers with 200 units each were used, here with rectified linear unit (ReLU, $\max(0, x)$) activations. In the output layer the parameters corresponding to the mean were left unchanged, while the variance parameters were passed through the exponential function. As prior distribution for the latent variables a centred isotropic Gaussian distribution was chosen. 

For all HMCVI experiments the leapfrog method was applied and the step size learnt (constrained to be positive). In experiments without partial momentum update ($\alpha = 0$ fixed) the reverse momentum model $r_V$ and final momentum model $r_\textrm{final}$ were joined into a single model as explained in section \ref{sec:SimplificationWithoutPartialMomentumUpdate}. This model was like the initial encoder model, but with the position and the time step as additional inputs. If partial momentum updates were included, the final momentum model was as in the previous case, but the reverse momentum model $r_V$ was a separate network with the updated momentum as an additional input and otherwise the same specifications as before (see section \ref{sec:TransitionDensitiesNoAcceptance}).

Where an acceptance step was included, either the converged chain approximation ("simple") derived in section~\ref{sec:TransDensitiesWithAcceptReverse} was used for the reverse acceptance probability $\mathbb{P}(A = 1|S_t = s_t, t, x)$ or a neural network was trained ("NN") for it. The output of this neural network, whose final layer was passed through the $\tanh$ function, was added to the converged chain approximation and then clipped to be in $[0, 1]$. The network took the current state, the time step and the observed variables as inputs and consisted of two hidden layers with 200 units each and ReLU activations.

For the canonical momentum distribution, which also specifies the kinetic energy, a zero mean multivariate normal distribution with diagonal covariance matrix was assumed throughout. For the diagonal entries three choices were compared: They were either set to 1 ("Identity") or learnt globally ("Global") or specified by a neural network ("NN"), taking the observed variables as input. In the second case the exponential function was applied to unconstrained parameters to ensure positivity. The neural network in the third case had a single hidden layer with 200 units and a ReLU activation and the exponential function as output transfer.

All parameters were independently initialized from a Gaussian distribution $N(0, 0.01)$. In HMCVI experiments the generative model and initial encoder model were then copied from a previously trained VAE (the same for all HMCVI experiments with the same number of latent variables). With this initialization the HMCVI methods showed much better training results than with fully random initialization.

\subsection{Model comparison}

\begin{table*}[ht]
\centering
\input{figures/tableResults}
\caption{Comparison of the obtained lower bound and marginal log-likelihood estimates for different HMCVI configurations with a 2-dimensional (top) and a 20-dimensional latent space (bottom). \#HMC and \#LF give the number of used HMC and leapfrog steps respectively. The fifth column indicates, whether partial momentum updates were permitted. The sixth column gives the strategy used for the covariance matrix $M$ of the canonical momentum distribution and the seventh column, whether the acceptance step was included and, if so, what approach was used (as described in section \ref{sec:ModelSpecifications}). The last two columns report the lower bound $\mathcal{L_\textrm{aux}}$ and the estimated NLL on the test set.}
\label{tab:Results}
\end{table*}

We maximized the lower bound for various different setups of the HMCVI framework. Table~\ref{tab:Results} shows the results obtained with a two-dimensional latent space (see appendix~\ref{app:LatentVisualizations} for some visualizations) and with a 20-dimensional latent space. The NLL estimates given were obtained using importance sampling with 5000 samples (described in appendix~\ref{app:NLLestimateImportSampling}).

From comparing the results, obtained using only a parametric posterior approximation (Basic VI 2D and 20D), to the HMCVI results it is obvious, that any additional HMC steps greatly improve the estimation quality. 

For the two-dimensional latent space we see that increasing the length of the simulated trajectory improves the results and that resampling the momentum more frequently (i.e.\ performing more HMC steps) is also beneficial (compare HMCVI 1-4). From the nature of HMC both of these observations are to be expected, since longer trajectories allow further movement through the latent space and hence better exploration. Likewise, more HMC steps implies a longer Markov chain, which should thus be closer to convergence. A more intuitive explanation of the second observation is, that initially the simulated particles may have high potential energies and move down the potential energy landscape increasing their kinetic energy. If their large built-up kinetic energy is then reduced by the resampling of the momentum, they can not move out of the potential energy basin they have slid into. Conversely, if there is a less frequent resampling of the momentum, their built-up momentum may carry them out of the basin again on the other side, so that their potential energy has not decreased as much and correspondingly their joint likelihood $p(x, z)$ has not increased as much (compare figures~\ref{fig:HMC_MOTION_1hmc_12lf} and \ref{fig:HMC_MOTION_3hmc_04lf}).

Interestingly, for the 20-dimensional latent space the bound worsens in our experiments, when the momentum is resampled more frequently, while the estimated NLL improves (see HMCVI 11-13). So w.r.t.\ the real target, the NLL, more HMC steps are positive, but this is not reflected in the bound. An explanation for this phenomenon could be that the auxiliary reverse model is not flexible enough to capture the additional reverse densities (introduced by the addition of HMC steps) as tightly, leading to a poorer bound.

Allowing partial momentum updates and the covariance matrix to depend on the observed variables further improved the performance as expected (HMCVI 5, 7, 14 and 16). With a two-dimensional latent space their combination produced in the best performing model (HMCVI 8). For the 20-dimensional latent space, the combination (HMCVI 17) yielded the best bound, but not the best NLL estimate. Fixing the covariance matrix to be the identity (HMCVI 6 and 15) performed worse than learning it globally for the 20-dimensional case, but no different for the two-dimensional case. Understandably, with two dimensions a global rescaling is unlikely to change much.

For the two-dimensional latent space, including the acceptance step returned worse results, but as to be expected the more complicated reverse probability model (HMCVI 10) outperformed the approach, where the chain was assumed to have already converged (HMCVI 9). The weaker performance of HMCVI with acceptance step in this case is probably due to the fact, that the short chains being used here have not nearly converged to their invariant distribution yet. Therefore, the reduced mixing due to the rejection of proposals outweighs possible gains from the improved posterior approximation, since only with the acceptance step the chain will actually converge to the true posterior. 

A different picture, however, presents itself for the 20-dimensional latent space: Again the lower bound is worse, when the acceptance step is included (HMCVI 18 and 19), but regarding the NLL estimate the models learnt with the acceptance step outperform all other models. This means that the inclusion of the acceptance step improved the quality of the VAE. This indicates, that in the larger latent space it is beneficial to reject some proposed transitions in order to obtain a better approximation of the posterior and this improved approximation allows a better decoder to be learnt. The poor quality of the bound is presumably due to the lacking flexibility of the reverse model, which has to deal with more noise and more complicated distributions, if the acceptance step is included (see section~\ref{sec:TransDensitiesWithAcceptReverse}). By combining the acceptance step with the input-dependent kinetic energy (HMCVI 20) the learnt model could be further improved as expected.