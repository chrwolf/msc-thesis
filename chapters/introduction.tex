\section{Introduction}

In modern data analysis probabilistic graphical models have emerged as a powerful and intuitive tool to capture and reveal hidden structures present in the data. Training and interpreting these models requires inferring the hidden representation of the observed data under the model. In many state-of-the-art graphical model approaches this key task is performed based on variational inference, a method converting complex inference problems into high-dimensional optimization problems \parencite{Jordan1999}. For instance \parencite{Hoffman2013} follow this approach for large scale text-to-topic models and \parencite{Gregor2015, Rezende2014, Kingma2014} apply it to the generation of images. 

Variational inference approximates the intractable true posterior distribution by the best-fitting candidate from a fixed family of distributions. While this makes the approximation procedure very fast, the restriction to a usually quite limited family of distributions means, that often the true posterior is only poorly approximated. This in turn hampers the training and final performance of the graphical model. Many suggestions for broader families of candidate distributions have been put forward allowing for more complicated approximations. A powerful framework, unifying several previous approaches, is the work by \parencite{Rezende2015} on normalizing flows. Here arbitrarily complicated distributions are generated by applying a sequence of invertible mappings to a simple initial distribution. An interesting example for such a normalizing flow is the Hamiltonian variational inference method derived by \parencite{Salimans2014}, where steps of the Hamiltonian Monte Carlo (HMC) algorithm are used to transform the initial distribution. Since the HMC algorithm generates a Markov chain converging to the true posterior, this extension to variational inference is particularly appealing, because the generated family of distributions is guaranteed to contain the true posterior (provided enough steps are taken). However, \parencite{Salimans2014} left out the acceptance step of the HMC algorithm, so that convergence to the true posterior is no longer ensured and the true posterior need not be within the generated distribution family.

In this work we exploit the structure of the HMC algorithm to derive the variational lower bound for the case, where a distribution is transformed by steps of the full HMC algorithm including the acceptance step. By doing so we regain the asymptotic guarantee of a perfect approximation. Additionally we present two extensions to the HMC algorithm, which can be included in the approximation procedure and speed up the convergence to the true posterior. We begin by revising Variational Inference, MCMC methods and the work by \parencite{Salimans2014} on their combination (section~\ref{sec:VIandMCMC}) as well as the Hamiltonian Monte Carlo algorithm (section~\ref{sec:HMC}). In section~\ref{sec:HMCVI} the aforementioned extensions are derived, before being applied in section~\ref{sec:Experiments}. In the final section~\ref{sec:ConclAndFuture} some ideas for further improvements are discussed.

%TODO In die Einleitung, dass Ergebnis vom Acceptance step auf unserem Datensatz nicht erfolgreich war