\section{Conclusion and future work}
\label{sec:ConclAndFuture}
In this work we analysed the previously suggested integration of the HMC algorithm into VI, focussing in particular on its theoretical foundations. By exploiting the structure of the HMC algorithm; we were able to include the Metropolis-Hastings acceptance step in the algorithm, which was previously left out, without adding any new variables. Only including this acceptance step in the HMC algorithm ensures the convergence of the chain to the true posterior. In our experiments the lower bound obtained when the acceptance step was included, was worse than without the acceptance step. However, w.r.t.\ the negative log-likelihood the models with acceptance step were superior (for a realistically sized latent space). The improved approximation of the posterior due to the inclusion of the acceptance step thus leads to a better variational auto-encoder being learnt. By increasing the flexibility of the reverse model this should also become apparent in the variational lower bound.

For the simplified case without the acceptance step, a better performance was also achieved by allowing partial momentum updates in the HMC algorithm, a generalization of the algorithm reported to be particularly beneficial for shorter-than-optimal trajectories. Further, we utilized the possibility of learning continuous parameters of the HMC algorithm as part of the maximization of the lower bound to make these parameters input-dependent. In this way, the algorithm is automatically adjusted to the current input. This lead to better results in our experiments, both with and without the acceptance step. In this work we only allowed the mass matrix to depend on the observed variables, but other parameters, such as the step size, could also be made input-dependent, promising further improvements.

While the HMCVI algorithm improves the density estimation, it also requires significantly more computational effort than basic VI, in particular, if the acceptance step is included. Making the algorithm computationally more efficient, for example by propagating approximate distributions instead of sampling individual points, would remove this drawback and also allow for longer chains leading to better convergence.

Another interesting question regarding HMCVI is the role of the auxiliary reverse model. Its existence and flexibility are necessary ingredients to make the lower bound tight and the other models train properly, but really the learnt reverse model is not needed once training is completed. In this sense, valuable training time is used for something unwanted. Understanding the function of this model further may yield computational speed-ups or better density estimation by removing apparent restrictions resulting from the current reverse model specifications.