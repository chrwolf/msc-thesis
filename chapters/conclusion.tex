\section{Conclusion and Future Work}
\label{sec:ConclAndFuture}
In this work we analysed the previously suggested integration of the Hamiltonian Monte Carlo algorithm into Variational Inference, focussing in particular on its theoretical foundations. By exploiting the structure of the HMC algorithm we were able to include the Metropolis-Hastings acceptance step in the algorithm, which was previously left out, without adding any new variables. Only including this acceptance step in the HMC algorithm ensures the convergence of the chain to the true posterior. In our experiments the lower bound obtained, when the acceptance step was included, was worse than that without the acceptance step, but w.r.t.\ the negative log-likelihood the learnt models with acceptance step outperformed the models without (for models with a realistically sized latent space). So the improved approximation of the posterior due to the inclusion of the acceptance step, even for the short chains used here, lead to a better variational auto-encoder being learnt. By increasing the flexibility of the reverse model this should also become apparent in the variational lower bound.

For the simplified case without the acceptance a better performance was also achieved by integrating partial momentum updates in the HMC algorithm, a generalization of the algorithm reported to be particularly beneficial for shorter-than-optimal chains. Further we utilized the possibility of learning continuous parameters of the HMC algorithm as part of the maximization of the lower bound to make these parameters input dependent. By doing this the algorithm can adapt to the current input, which lead to a better fitting model in our experiments. In this work we only allowed the mass matrix to be dependent on the observed variables, but other parameters, such as the step size, could also be made input dependent, promising further improvements. Combining this with the acceptance step promises to enhance the benefits of the acceptance step, since these modifications should promote faster convergence.

While the density estimation could be improved by using our HMCVI algorithm, it also requires significantly more computational effort than basic VI, in particular, if the acceptance step is included. Making the algorithm computationally more efficient, for example by propagating approximate distributions instead of sampling individual points, would remove this drawback and also allow for longer chains leading to better convergence.

Another interesting question regarding HMCVI is the role of the auxiliary reverse model. Its existence and flexibility are necessary ingredients to make the lower bound tight and the other models train properly, but really the learnt reverse model is not needed once training is completed. In this sense valuable training time is used for something unwanted. Understanding the function of this model further may yield computational speed-ups or better density estimation by removing apparent restrictions resulting from the current reverse model specifications.