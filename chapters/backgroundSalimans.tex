\section{Variational Inference and MCMC}
\label{sec:VIandMCMC}
\subsection{Variational Inference}

In a probabilistic model $p(x, z)$ with missing or latent variables $z$ (possibly parameters in a Bayesian setting) the quantity of interest for inference problems is the marginal likelihood $p(x) = \int p(x, z) dz$. This integral is usually intractable and only a lower bound $\mathcal{L}$ to its value can be obtained using the variational principle:
\begin{equation}
\begin{split}
\log p(x) &\geq \log p(x) - D_{KL} \left( q_{\theta}(z|x) || p(z|x) \right) \\
			   &=  \E_{q_{\theta}(z|x)} \left[ \log p(x, z) - \log q_\theta(z|x) \right] \eqqcolon \mathcal{L}
\end{split}
\end{equation}
This requires the approximation of the true posterior $p(z|x)$, which is usually also intractable, by a parametrized density $q_{\theta}(z|x)$. By maximizing $\mathcal{L}$ with respect to the parameters $\theta$, the KL-divergence between the true and the approximate posterior is minimized and reaches its minimum, when the approximation equals the true posterior. In this case, $\log p(x) = \mathcal{L}$. From this derivation it is clear, that the success of this method, known as variational inference (VI), strongly depends on the approximation capacity of $q_\theta$.

\subsection{MCMC}
\label{sec:MCMC}

% Another way to approximate the intractable posterior is by repeatedly sampling from it using Markov Chain Monte Carlo (MCMC) methods. In MCMC first a random state $s_0$ is drawn from some initial distribution $q_0(s|x)$, where usually the state is the latent variable $z$. Then a stochastic transition operator $s_{t} \sim q(s_t|s_{t-1}, x)$ is applied repeatedly, producing a Markov chain $(s_t)_{t \in \mathbb{N}}$. By appropriate choice of the transition density $q$ a Markov chain can usually be constructed, which has two key properties: Firstly its stationary distribution $p^*(s)$ is the intractable posterior distribution $p(z|x)$ (or $p(z|x)$ can be derived from it) and secondly the chain converges to its stationary distribution (\cite{Roberts2004}). 

A widely used method to approximate intractable distributions is to repeatedly sample from them using Markov Chain Monte Carlo (MCMC) methods. To draw samples from an arbitrary target distribution with density $f_\textrm{target}(s)$ using MCMC, first a random state $s_0$ is drawn from some initial distribution $q_0(s)$. Then, a stochastic transition operator $s_{t} \sim q(s_t|s_{t-1})$ is applied repeatedly, producing a Markov chain $(s_t)_{t \in \mathbb{N}}$. By appropriate choice of the transition density $q$ a Markov chain can be constructed, which under minor regularity conditions has two key properties: Firstly its stationary distribution is the target distribution $f_\textrm{target}$ and secondly the chain converges to its stationary distribution \parencite{Roberts2004}. Therefore, by running such a chain for a sufficient number of steps, a sample from the target distribution can be obtained. However, the number of steps required is unknown a priori and may be very large.

The most common method for constructing such a Markov chain is the Metropolis-Hastings algorithm, where the transition is constructed in two steps: First a new proposed state $\tilde{s}_t$ is sampled from a proposal distribution $\tilde{q}(\tilde{s}_t|s_{t-1})$. In the second step, the acceptance step, this proposal is then accepted as the new state with probability 
\begin{equation} \label{eq:Metropolis-Hastings}
\begin{split}
p_{\textrm{accept}}&(s_{t-1}, \tilde{s}_t) \\
&= \min \left[ 1, \frac{f_\textrm{target}(\tilde{s}_t)}{f_\textrm{target}(s_{t-1})} \cdot \frac{\tilde{q}(s_{t-1}|\tilde{s}_t)}{\tilde{q}(\tilde{s}_t|s_{t-1})} \right],
\end{split}
\end{equation}
in which case we set $s_t = \tilde{s}_t$. Otherwise, the current state is kept, so $s_t = s_{t-1}$. It can be shown, that this indeed produces a Markov chain with the required properties \parencite{Roberts2004}. 

It is important to note that the target distribution density appears both in the enumerator and denominator, so we do not need the target distribution function to be normalized. This is essential for the use of MCMC with Bayesian inference, since Bayes's Theorem states $p(z|x) \propto p(x|z) \cdot p(z)$ with the usually intractable normalization factor $p(x)$.

\subsection{Combining variational inference and MCMC}
\label{sec:MCVI}
For sampling from the intractable posterior $p(z|x)$ via MCMC, we could choose the unobserved variable $z$ as state and the exact posterior $p(z|x)$ as target distribution. In contrast to the parametrized distribution $q_\theta(z|x)$ in VI, this gives us an asymptotically exact approximation of the posterior. However, it is also computationally expensive and does not offer an explicit objective function (which is e.g.\ needed for training the generative model $p(x, z)$).

To integrate the adaptiveness of MCMC into VI \textcite{Salimans2014} have proposed a powerful combination of these two methods, which they call Markov Chain Variational Inference (MCVI). The idea is to interpret the Markov Chain obtained in MCMC as a variational approximation $q(z_0, \dots, z_T|x) = q_{0}(z_0|x) \cdot \prod_{t=1}^T q(z_t|z_{t-1}, x)$. Due to the additional variables $y = (z_0, \dots, z_{T-1})$ ($z_T$ corresponds to the output of standard VI), the lower bound must be modified:
\begin{equation}
\begin{split}
\log &p(x) \geq \mathcal{L} \\
	   &\geq \mathcal{L} - \E_{q(z_T|x)} \big[ D_{KL} \left( q(y |z_T, x) || r(y|z_T, x) \right) \big] \\
	   &=  \E_{q(y, z_T|x)} \big[ \log p(x, z_T) + \log r(y |z_T, x) \\
	   &\qquad\qquad\qquad  - \log q(y, z_{T} |x) \big] \\
	   &\eqqcolon \mathcal{L}_{\textrm{aux}},
\end{split}
\end{equation}
where $r(y|z_T, x)$ is an auxiliary distribution to be learnt as an approximation of the intractable $q(y |z_T, x)$. 

Due to the Markov chain structure of the \textit{forward} distribution $q(z_1, \dots, z_T|z_0, x) = \prod_{t=1}^T q(z_t|z_{t-1}, x)$, a natural choice for the auxiliary \textit{reverse} distribution is to mimic this structure, i.e.\ to assume $r(z_0, \dots, z_{T-1} |z_T, x) = \prod_{t=1}^T r(z_{t-1}|z_t, t, x)$. It is worth noting that conversely to the forward model, where the transitions should be independent of the step number (as in MCMC), the reverse model may use the step number to achieve a better fit. This allows the reverse model to capture the decreasing bias due to the initial distribution $q_0(z_0|x)$. In this case, the auxiliary lower bound can be rewritten as
\begin{equation} \label{eq:MCVIAuxLowerBound}
\begin{split}
\mathcal{L}_{\textrm{aux}} &= \E_{q(z_0, \dots, z_T|x)} \left[ \log p(x, z_T) - \log q(z_0|x) \right] \\
& \quad + \sum_{t=1}^T \E_{q(z_0, \dots, z_T|x)} \big[ \log r(z_{t-1}|z_t, t, x) \\
& \quad\qquad\qquad\qquad\qquad\; - \log q(z_t|z_{t-1}, x)  \big] 
\end{split}
\end{equation}

Provided that the random variables within the expectations are differentiable w.r.t.\ the parameters, an efficient Monte Carlo estimate of the gradient of the lower bound w.r.t.\ the parameters can be computed \parencite{Kingma2014, Rezende2014}. This gradient estimate can then be used to train the forward and the reverse model (and if applicable the generative model $p(x, z)$) using gradient-based stochastic optimization algorithms such as Adam \parencite{Kingma2015}.
